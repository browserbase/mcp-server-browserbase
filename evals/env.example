# Browserbase MCP Server Evaluation Environment Variables

# Required: Browserbase API credentials
# Get these from https://www.browserbase.com/dashboard
BROWSERBASE_API_KEY=bb_api_key_your_key_here
BROWSERBASE_PROJECT_ID=bb_project_id_your_project_id_here

# Required: Anthropic API key for Claude (workflow execution)
# Get this from https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-your_anthropic_key_here

# Optional: OpenAI API key for LLM judge (advanced tests only)
# Get this from https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your_openai_key_here

# Optional: Override default timeouts (in milliseconds)
# EVAL_TIMEOUT=60000

# Optional: Override default models
# EVAL_JUDGE_MODEL=gpt-4o
# EVAL_PASS_THRESHOLD=0.8

# Usage Instructions:
# 1. Copy this file to .env: cp evals/env.example .env
# 2. Replace the placeholder values with your actual API keys
# 3. Run: source .env (or use direnv/dotenv)
# 4. Run tests: npm test 